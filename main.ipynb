{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 256\n",
    "num_epochs = 100\n",
    "patch_size = 32\n",
    "contrastive_batch_size = 256\n",
    "batch_size = 8\n",
    "classes = 2\n",
    "learning_rate = 1e-3\n",
    "\n",
    "data_dir = '/home/louis/Documents/project/PatchCL-MedSeg/0_data_dataset_voc_950_kidney/'\n",
    "output_dir = '/home/louis/Documents/project/pixel-contrastive-segmentation/dataset/splits/kidney/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader\n",
    "from pixel_level_contrastive_learning import PixelCL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset_kidney import BaseDatasets  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.26.4\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================================================\n",
      "number of labeled_dataset:  285\n",
      "number of unlabeled_dataset:  570\n",
      "number of val_dataset:  95\n",
      "===========================================================\n"
     ]
    }
   ],
   "source": [
    "# Define transformations if needed\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((img_size, img_size)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "IMG_folder_path = data_dir \n",
    "msk_folder_path = data_dir\n",
    "# Load file lists\n",
    "with open(os.path.join(output_dir, \"1-3\", \"labeled.txt\"), 'r') as file:\n",
    "    labeled_files = [line.strip().split(' ') for line in file.readlines()]\n",
    "with open(os.path.join(output_dir, \"1-3\", \"unlabeled.txt\"), 'r') as file:\n",
    "    unlabeled_files = [line.strip() for line in file.readlines()]\n",
    "with open(os.path.join(output_dir, \"val.txt\"), 'r') as file:\n",
    "    val_files = [line.strip().split(' ') for line in file.readlines()]\n",
    "\n",
    "# # Create datasets and dataloaders\n",
    "labeled_dataset = BaseDatasets(labeled_files, IMG_folder_path, msk_folder_path, transform)\n",
    "unlabeled_dataset = BaseDatasets(unlabeled_files, IMG_folder_path, transform=transform)\n",
    "val_dataset = BaseDatasets(val_files, IMG_folder_path, msk_folder_path, transform)\n",
    "\n",
    "labeled_dataloader = DataLoader(labeled_dataset, batch_size=batch_size, shuffle=True)\n",
    "unlabeled_dataloader = DataLoader(unlabeled_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "print('===========================================================')\n",
    "print('number of labeled_dataset: ', len(labeled_dataset))\n",
    "print('number of unlabeled_dataset: ', len(unlabeled_dataset))\n",
    "print('number of val_dataset: ', len(val_dataset))\n",
    "print('===========================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import models\n",
    "from torchvision.models import ResNet50_Weights\n",
    "\n",
    "# Define the segmentation model based on ResNet\n",
    "class ResNetSegmentation(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        # Base ResNet model, without the final fully connected layer\n",
    "        original_resnet = models.resnet50(pretrained=True)\n",
    "        self.backbone = nn.Sequential(*list(original_resnet.children())[:-2])\n",
    "        \n",
    "        # Additional layers for segmentation\n",
    "        self.conv1x1 = nn.Conv2d(2048, num_classes, 1)\n",
    "        self.upsample = nn.Upsample(scale_factor=32, mode='bilinear', align_corners=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.conv1x1(x)\n",
    "        x = self.upsample(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/louis/anaconda3/envs/PCLS/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/louis/anaconda3/envs/PCLS/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([1, 2, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "num_classes = 2  # 例如 VOC2012 数据集的类别数\n",
    "model = ResNetSegmentation(num_classes).cuda()\n",
    "\n",
    "# 检查模型输出\n",
    "dummy_input = torch.randn(1, 3, 256, 256).cuda()  # 假设输入是 256x256 的图像\n",
    "dummy_output = model(dummy_input)\n",
    "print(\"Output shape:\", dummy_output.shape)  # 应该是 (1, num_classes, 256, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the unmodified ResNet-50 for PixelCL\n",
    "original_resnet = models.resnet50(pretrained=True).cuda()\n",
    "learner = PixelCL(\n",
    "    original_resnet,\n",
    "    image_size=256,\n",
    "    hidden_layer_pixel='layer4',\n",
    "    hidden_layer_instance=-2,\n",
    "    projection_size=256,\n",
    "    projection_hidden_size=2048,\n",
    "    moving_average_decay=0.99,\n",
    "    ppm_num_layers=1,\n",
    "    ppm_gamma=2,\n",
    "    distance_thres=0.7,\n",
    "    similarity_temperature=0.3,\n",
    "    alpha=1.0,\n",
    "    use_pixpro=True,\n",
    "    cutout_ratio_range=(0.6, 0.8)\n",
    ").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(list(model.parameters()) + list(learner.parameters()), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function including both supervised and unsupervised losses\n",
    "def train_epoch(model, learner, labeled_loader, optimizer, criterion, epoch):\n",
    "    model.train()\n",
    "    total_supervised_loss = 0\n",
    "    total_contrastive_loss = 0\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    model.to(device)\n",
    "    learner.to(device)\n",
    "\n",
    "    for imgs, masks in tqdm(labeled_loader, desc=f\"Epoch {epoch+1}\", leave=False):\n",
    "        imgs, masks = imgs.to(device), masks.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass for segmentation\n",
    "        outputs = model(imgs)\n",
    "        supervised_loss = criterion(outputs, masks)\n",
    "        total_supervised_loss += supervised_loss.item()\n",
    "\n",
    "        # Calculate the unsupervised PixelCL loss\n",
    "        contrast_loss = learner(imgs)\n",
    "        total_contrastive_loss += contrast_loss.item()\n",
    "\n",
    "        # Combine losses and backpropagate\n",
    "        loss = supervised_loss + contrast_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the moving average for the target encoder in PixelCL\n",
    "        learner.update_moving_average()\n",
    "\n",
    "    avg_supervised_loss = total_supervised_loss / len(labeled_loader)\n",
    "    avg_contrastive_loss = total_contrastive_loss / len(labeled_loader)\n",
    "    print(f\"Epoch [{epoch+1}], Supervised Loss: {avg_supervised_loss:.4f}, Contrastive Loss: {avg_contrastive_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1], Supervised Loss: 0.1139, Contrastive Loss: 1.6439\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2], Supervised Loss: 0.0367, Contrastive Loss: 1.0215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3], Supervised Loss: 0.0352, Contrastive Loss: 0.9271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4], Supervised Loss: 0.0340, Contrastive Loss: 0.8646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5], Supervised Loss: 0.0333, Contrastive Loss: 0.7586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6], Supervised Loss: 0.0326, Contrastive Loss: 0.5874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7], Supervised Loss: 0.0323, Contrastive Loss: 0.5377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8], Supervised Loss: 0.0321, Contrastive Loss: 0.3693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9], Supervised Loss: 0.0317, Contrastive Loss: 0.1773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10], Supervised Loss: 0.0317, Contrastive Loss: 0.2566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    train_epoch(model, learner, labeled_dataloader, opt, nn.CrossEntropyLoss(), epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the improved segmentation model\n",
    "torch.save(model.state_dict(), 'improved-resnet-segmentation.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def calculate_iou(pred, target, n_classes):\n",
    "    ious = []\n",
    "    # Convert softmax predictions to class indexes if not already\n",
    "    pred = torch.argmax(pred, dim=1) if pred.shape[1] > 1 else pred.squeeze(1)\n",
    "    \n",
    "    if target.shape[1] > 1:\n",
    "        # Assuming target is one-hot encoded\n",
    "        target = torch.argmax(target, dim=1)\n",
    "    \n",
    "    # Flatten the arrays to calculate IoU on a per-pixel basis\n",
    "    pred = pred.view(-1)\n",
    "    target = target.view(-1)\n",
    "\n",
    "    for cls in range(n_classes):\n",
    "        pred_inds = (pred == cls)\n",
    "        target_inds = (target == cls)\n",
    "        \n",
    "        # Calculate Intersection and Union\n",
    "        intersection = (pred_inds & target_inds).sum().item()\n",
    "        union = pred_inds.sum().item() + target_inds.sum().item() - intersection\n",
    "        \n",
    "        if union == 0:\n",
    "            ious.append(float('nan'))  # Avoid division by zero\n",
    "        else:\n",
    "            ious.append(intersection / union)\n",
    "    \n",
    "    return np.nanmean(ious)\n",
    "\n",
    "def iou_pytorch(outputs: torch.Tensor, labels: torch.Tensor):\n",
    "    SMOOTH = 1e-6\n",
    "    \n",
    "    # Convert output probabilities to binary predictions\n",
    "    outputs = torch.sigmoid(outputs)  # Assuming outputs are logits from the model; remove if already probabilities\n",
    "    outputs = (outputs > 0.5).float()  # Threshold the probabilities to create binary predictions\n",
    "    \n",
    "    # Flatten the tensors to simplify the intersection/union computation\n",
    "    outputs = outputs.view(outputs.shape[0], -1)\n",
    "    labels = labels.view(labels.shape[0], -1)\n",
    "    \n",
    "    # Compute the intersection and union\n",
    "    intersection = (outputs * labels).sum(1)\n",
    "    union = (outputs + labels).sum(1) - intersection\n",
    "    \n",
    "    # Compute IoU and handle cases where the union is 0\n",
    "    iou = (intersection + SMOOTH) / (union + SMOOTH)\n",
    "    \n",
    "    # You can threshold IoU values here if needed (e.g., for metric computations)\n",
    "    return iou.mean()\n",
    "\n",
    "\n",
    "def validate_model(model, val_loader, criterion, device, num_classes):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_iou = 0\n",
    "    count = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for imgs, masks in tqdm(val_loader, desc=\"Validating\", leave=False):\n",
    "            imgs, masks = imgs.to(device), masks.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, masks)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Calculate IoU - ensure outputs and masks are compatible\n",
    "            iou = iou_pytorch(outputs, masks)\n",
    "            total_iou += iou.item()  # Sum IoU for averaging\n",
    "            count += 1\n",
    "    \n",
    "    avg_loss = total_loss / len(val_loader)\n",
    "    avg_iou = total_iou / count if count > 0 else 0\n",
    "    return avg_loss, avg_iou\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.0342\n",
      "Mean IoU: 0.9811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# 设置设备\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# 计算验证集上的平均损失和 IoU\n",
    "val_loss, val_iou = validate_model(model, val_dataloader, nn.CrossEntropyLoss(), device, num_classes)\n",
    "\n",
    "print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "print(f\"Mean IoU: {val_iou:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PCLS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
